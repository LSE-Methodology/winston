{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Winston HPC User Guide","text":"<p>Welcome to Winston \u2014 a small HPC cluster for running CPU and GPU workloads via Slurm.</p> <p>Use the navigation on the left to get started:</p> <ul> <li>Accessing Winston (SSH + keys + VPN)</li> <li>Running jobs (batch, interactive, GPU)</li> <li>Managing software (Spack + Miniconda)</li> <li>Moving data (scp/rsync + GUI tools)</li> </ul> <p>New users</p> <p>If you\u2019re not sure where to start: read Getting started \u2192 Accessing Winston, then Slurm jobs \u2192 Overview. If you have not had an account created yet, please email Tom @ t.robinson7@lse.ac.uk to gain access.</p>"},{"location":"policies/","title":"Policies and good practice","text":""},{"location":"policies/#fair-usage","title":"Fair usage","text":"<ul> <li>Do not run intensive workloads on the login node (small tests are fine).</li> <li>Use Slurm for anything requiring &gt;2 cores or &gt;4GB RAM.</li> </ul>"},{"location":"policies/#software-installs","title":"Software installs","text":"<ul> <li>Keep Spack builds under <code>/data/spack/users/$USER</code>.</li> <li>Do not modify anything under <code>/opt/spack</code>.</li> <li>Remove unused Conda environments occasionally.</li> <li>Export environments for reproducibility:</li> <li><code>conda env export &gt; env.yml</code></li> <li><code>spack find --format '{name}@{version}' &gt; spack-env.txt</code></li> </ul>"},{"location":"policies/#time-limits","title":"Time limits","text":"<ul> <li>Always set <code>#SBATCH --time</code> realistically.</li> <li>Jobs exceeding walltime will be terminated.</li> </ul>"},{"location":"getting-started/access/","title":"Accessing Winston","text":"<p>Winston is accessible via SSH from within the LSE network.</p>"},{"location":"getting-started/access/#ssh-from-within-the-lse-network","title":"SSH from within the LSE network","text":"<pre><code>ssh user@ip.address\n</code></pre> <p>Note</p> <p>Replace <code>user</code> and <code>ip.address</code> with your Winston username and the IP address provided when your account was created.</p>"},{"location":"getting-started/access/#access-from-outside-lse-vpn","title":"Access from outside LSE (VPN)","text":"<p>To access Winston off-campus, connect to the LSE VPN and use SSH as normal.</p>"},{"location":"getting-started/file-transfer/","title":"Transferring files","text":"<p>You can move data to/from Winston using either GUI tools or command-line tools.</p>"},{"location":"getting-started/file-transfer/#gui-option","title":"GUI option","text":"<p>A tool like CyberDuck can transfer files to your user space on Winston using:</p> <ul> <li>your username</li> <li>your authentication method (password or key)</li> <li>the Winston host/IP</li> </ul>"},{"location":"getting-started/file-transfer/#command-line-scp","title":"Command line: <code>scp</code>","text":"<p>Copy local \u2192 Winston:</p> <pre><code>scp -r &lt;path/to/local/files&gt; &lt;username&gt;@158.143.30.9:&lt;remote/path&gt;\n</code></pre> <p>Copy Winston \u2192 local (reverse direction):</p> <pre><code>scp -r &lt;username&gt;@winston.ip.address:&lt;remote/path&gt; &lt;path/to/local/folder&gt;\n</code></pre> <p>Notes:     - <code>-r</code> recursively copies directories</p>"},{"location":"getting-started/file-transfer/#command-line-rsync-recommended-for-lots-of-files","title":"Command line: rsync (recommended for lots of files)","text":"<p>rsync is faster for large transfers and incremental updates (only changed files are sent).</p> <pre><code>rsync -av &lt;path/to/local/files&gt; &lt;username&gt;@&lt;ip_address&gt;:&lt;remote/path&gt;\n</code></pre> <p>Common flags:     - <code>-a</code> preserve metadata (permissions, timestamps)     - <code>-v</code> verbose output     - add <code>-z</code> to compress during transfer (useful over slower links)</p>"},{"location":"getting-started/ssh-tips/","title":"SSH tips","text":""},{"location":"getting-started/ssh-tips/#make-a-shortcut-host-entry","title":"Make a shortcut host entry","text":"<p>If you don\u2019t want to type the IP every time, add a host alias to your ssh config:</p> <pre><code>nano ~/.ssh/config\n</code></pre> <p>Then add:</p> <pre><code>Host winston\n    HostName &lt;ip_address&gt;\n    User &lt;username&gt;\n</code></pre> <p>Then connect with:</p> <pre><code>ssh winston\n</code></pre>"},{"location":"reference/commands/","title":"Useful commands","text":""},{"location":"reference/commands/#slurm-essentials","title":"Slurm essentials","text":"<pre><code>squeue # see running/queued jobs\nsbatch script.sh # submit job\nsalloc --mem=10G --cpus-per-task=4 --time=01:00:00 # reserve interactive resources\nscancel &lt;JOBID&gt; # cancel job\n</code></pre> <p>Node details:</p> <pre><code>scontrol show node &lt;node&gt; # see node description\n</code></pre> <p>Spack essentials:</p> <pre><code>spack search &lt;name&gt;\nspack install &lt;pkg&gt;\nspack load &lt;pkg&gt;\nspack find\nspack unload &lt;pkg&gt;\n</code></pre> <p>Conda essentials:</p> <pre><code>conda create -n myenv python=3.11\nconda activate myenv\nconda deactivate\n</code></pre>"},{"location":"reference/resources/","title":"Cluster resources","text":"<p>Approximate available resources:</p> <ul> <li>~192 vCPUs</li> <li>~250GB RAM</li> <li>2 \u00d7 RTX 6000 Ada (48GB VRAM each)</li> </ul>"},{"location":"reference/resources/#memory-defaults","title":"Memory defaults","text":"<p>By default, Slurm jobs are assigned 1GB RAM per CPU core unless you request more.</p>"},{"location":"reference/resources/#checking-availability-of-resources","title":"Checking availability of resources","text":"<pre><code>winfo\n</code></pre>"},{"location":"slurm/batch-jobs/","title":"Batch jobs","text":"<p>Batch jobs are submitted with <code>sbatch</code>. You describe what you need (time, memory, CPUs), and Slurm schedules it.</p>"},{"location":"slurm/batch-jobs/#example-cpu-batch-job","title":"Example: CPU batch job","text":"<p>Create a script, e.g. <code>job.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_python_job\n#SBATCH --output=output_%j.log\n#SBATCH --error=error_%j.log\n#SBATCH --time=01:00:00        # Max walltime (hh:mm:ss)\n#SBATCH --mem=10G              # Memory per job\n#SBATCH --cpus-per-task=4      # Number of CPU cores\n\npython my_script.py\n</code></pre> <p>Submit it:</p> <pre><code>sbatch job.sh\n</code></pre> <p>Check status:</p> <pre><code>squeue\n</code></pre> <p>Walltime default</p> <p>If you do not set --time, the current default is 12 hours. Jobs that exceed their walltime will be terminated by the scheduler.</p>"},{"location":"slurm/gpu-jobs/","title":"GPU jobs","text":"<p>Use the GPU partition for CUDA workloads.</p>"},{"location":"slurm/gpu-jobs/#partitions","title":"Partitions","text":"<ul> <li><code>winston</code> (default): general CPU queue (use for most jobs)</li> <li><code>winston-gpu</code>: for GPU jobs (CUDA) and privileged access to GPU resources</li> </ul> <p>GPU partition limits</p> <p>The <code>winston-gpu</code> partition has 4 dedicated CPUs and 50GB exclusive RAM. Additional resources may be available depending on cluster load, but are not guaranteed.</p>"},{"location":"slurm/gpu-jobs/#example-gpu-batch-job","title":"Example: GPU batch job","text":"<p>Create a batch script <code>my_gpu_job.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_gpu_job\n#SBATCH --partition=winston-gpu\n#SBATCH --gres=gpu:1\n#SBATCH --time=02:00:00\n#SBATCH --mem=20G\n#SBATCH --cpus-per-task=4\n#SBATCH --output=output_%j.log\n\npython train.py\n</code></pre> <p>Submit with:</p> <pre><code>sbatch my_gpu_job.sh\n</code></pre>"},{"location":"slurm/interative-jobs/","title":"Interactive jobs","text":"<p>Interactive jobs let you reserve resources and work \u201clive\u201d on a compute node.</p>"},{"location":"slurm/interative-jobs/#request-an-interactive-allocation","title":"Request an interactive allocation","text":"<pre><code>salloc --mem=10G --cpus-per-task=4 --time=01:00:00\n</code></pre> <p>Once allocated, run your commands as normal (on the allocated node).</p> <p>Tip</p> <p>If you need interactive work frequently, consider using tmux so sessions survive disconnects.</p>"},{"location":"slurm/overview/","title":"Slurm overview","text":"<p>Winston uses Slurm to queue jobs and allocate compute resources fairly.</p> <p>You can use Winston in two main ways:</p> <ol> <li>Batch jobs (preferred): submit a script; Slurm runs it when resources are available.</li> <li>Interactive jobs: reserve resources, then work on a compute node.</li> </ol> <p>Do not run heavy work on the login node</p> <p>Small tests are fine, but intensive workloads must run via Slurm. Login nodes are limited to 2 CPU cores and 4GB RAM.</p>"},{"location":"slurm/overview/#quick-commands","title":"Quick commands","text":"<pre><code>squeue           # view jobs\nscancel &lt;JOBID&gt;  # cancel a job\nwinfo            # view allocated resources / cluster usage (local helper)\n</code></pre>"},{"location":"software/conda/","title":"Miniconda","text":"<p>Miniconda is recommended for user-level environments (Python, R packages, data-science tools).</p>"},{"location":"software/conda/#first-time-install","title":"First-time install","text":"<pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3\nsource ~/miniconda3/etc/profile.d/conda.sh\nconda config --set auto_activate_base false\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n</code></pre>"},{"location":"software/conda/#shell-setup","title":"Shell setup","text":"<p>Add to <code>~/.bashrc</code>:</p> <pre><code>if [ -f \"$HOME/miniconda3/etc/profile.d/conda.sh\" ]; then\n    source \"$HOME/miniconda3/etc/profile.d/conda.sh\"\nfi\n</code></pre> <p>Reload your bash profile:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Create and use an environment:</p> <pre><code>conda create -n py311 python=3.11 -y\nconda activate py311\nconda install numpy pandas matplotlib\n</code></pre> <p>Deactivate:</p> <pre><code>conda deactivate\nReproducibility\nconda env export &gt; env.yml\n</code></pre>"},{"location":"software/overview/","title":"Software management overview","text":"<p>Winston is configured for user-managed software installations. You do not need administrator privileges (<code>sudo</code>) to install your own tools.</p> <p>Two supported approaches:</p> <ul> <li>Spack: compiled system software (R, compilers, MPI, scientific libraries)</li> <li>Miniconda: user-level environments (Python/R packages, data science stacks)</li> </ul>"},{"location":"software/overview/#system-overview","title":"System overview","text":"Layer Tool Purpose Shared base layer Spack (<code>/opt/spack</code>) Core system software (R, compilers, MPI, libraries) User build area <code>/data/spack/users/$USER</code> Your Spack-built software install tree User environments Miniconda (<code>~/miniconda3</code>) Python/R environments + packages Scheduler Slurm Runs jobs on compute nodes"},{"location":"software/slurm-with-envs/","title":"Using Spack/Conda inside Slurm jobs","text":"<p>You can use Spack and Conda together in the same job.</p>"},{"location":"software/slurm-with-envs/#example-batch-script","title":"Example batch script","text":"<p><code>job.sbatch</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --time=00:05:00\n#SBATCH --ntasks=1\n#SBATCH --output=output_%j.log\n\n# Set up Spack + Conda\nsource /opt/spack/share/spack/setup-env.sh\nsource ~/miniconda3/etc/profile.d/conda.sh\n\nconda activate py311\nspack load r\n\nRscript myscript.R\n</code></pre> <p>Submit:</p> <pre><code>sbatch job.sbatch\n</code></pre> <p>Tip</p> <p>Spack is great for optimized compiled libraries; Conda is great for high-level packages.</p>"},{"location":"software/spack/","title":"Spack","text":"<p>Spack manages compiled system-level software such as R, compilers, MPI, and scientific libraries.</p>"},{"location":"software/spack/#shell-setup","title":"Shell setup","text":"<p>Add to your <code>~/.bashrc</code> (if not already present):</p> <pre><code># Load Spack\nif [ -f /opt/spack/share/spack/setup-env.sh ]; then\n    source /opt/spack/share/spack/setup-env.sh\nfi\n</code></pre> <p>Reload your shell:</p> <pre><code>source ~/.bashrc\n</code></pre>"},{"location":"software/spack/#installing-and-loading-packages","title":"Installing and loading packages","text":"<p>Example:</p> <pre><code>spack install r@4.4.1\nspack load r@4.4.1\nR --version\n</code></pre> <p>Notes:     - Builds go under: /data/spack/users/$USER/opt/spack     - Remove availability in your shell with: <code>spack unload &lt;pkg&gt;</code>     - List installed packages: <code>spack find</code></p> <p>Note</p> <p>If a package already exists on the system, Spack may use a prebuilt binary automatically.</p>"}]}