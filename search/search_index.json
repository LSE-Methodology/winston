{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Winston HPC User Guide","text":"help@winston:~      \u2598    \u2597   \u258c\u258c\u258c\u258c\u259b\u258c\u259b\u2598\u259c\u2598\u259b\u258c\u259b\u258c   \u259a\u259a\u2598\u258c\u258c\u258c\u2584\u258c\u2590\u2596\u2599\u258c\u258c\u258c   LSE Methodology's HPC for running CPU/GPU workloads via Slurm.   Use the navigation on the left to get started."},{"location":"#quick-start","title":"Quick start","text":"<ol> <li>Get your username and host/IP from the admin.</li> <li>Connect via SSH (see Getting started \u2192 Accessing Winston).</li> <li>Run small tests on login, submit real work via Slurm.</li> <li>Use Software \u2192 Miniconda or Software \u2192 Spack to install tools.</li> </ol> <p>New users</p> <p>If you\u2019re not sure where to start: read Getting started \u2192 Accessing Winston, then Slurm jobs \u2192 Overview. If you don\u2019t yet have an account, email Tom at t.robinson7@lse.ac.uk.</p> <p>Host details</p> <p>For security, the Winston host/IP is shared privately during account setup.</p>"},{"location":"policies/","title":"Policies and good practice","text":""},{"location":"policies/#fair-usage","title":"Fair usage","text":"<ul> <li>Do not run intensive workloads on the login node (small tests are fine).</li> <li>Use Slurm for anything requiring &gt;2 cores or &gt;4GB RAM.</li> <li>Prefer short interactive sessions for debugging; submit long runs as batch jobs.</li> </ul>"},{"location":"policies/#software-installs","title":"Software installs","text":"<ul> <li>Keep Spack builds under <code>/data/spack/users/$USER</code>.</li> <li>Do not modify anything under <code>/opt/spack</code>.</li> <li>Remove unused Conda environments occasionally.</li> <li>Export environments for reproducibility:</li> <li><code>conda env export &gt; env.yml</code></li> <li><code>spack find --format '{name}@{version}' &gt; spack-env.txt</code></li> </ul>"},{"location":"policies/#time-limits","title":"Time limits","text":"<ul> <li>Always set <code>#SBATCH --time</code> realistically.</li> <li>Jobs exceeding walltime will be terminated.</li> </ul>"},{"location":"getting-started/access/","title":"Accessing Winston","text":"<p>Winston is accessible via SSH from within the LSE network.</p>"},{"location":"getting-started/access/#ssh-from-within-the-lse-network","title":"SSH from within the LSE network","text":"<pre><code>ssh &lt;username&gt;@&lt;host-provided-by-admin&gt;\n</code></pre> <p>Note</p> <p>Replace <code>&lt;username&gt;</code> and <code>&lt;host-provided-by-admin&gt;</code> with the details provided when your account was created. If you were given an SSH key, add <code>-i /path/to/key</code> to the command.</p>"},{"location":"getting-started/access/#access-from-outside-lse-vpn","title":"Access from outside LSE (VPN)","text":"<p>To access Winston off-campus, connect to the LSE VPN and use SSH as normal.</p>"},{"location":"getting-started/file-transfer/","title":"Transferring files","text":"<p>You can move data to/from Winston using either GUI tools or command-line tools.</p>"},{"location":"getting-started/file-transfer/#gui-option","title":"GUI option","text":"<p>A tool like Cyberduck (SFTP) can transfer files to your user space on Winston using:</p> <ul> <li>your username</li> <li>your authentication method (password or key)</li> <li>the Winston host (provided privately)</li> </ul>"},{"location":"getting-started/file-transfer/#command-line-scp","title":"Command line: <code>scp</code>","text":"<p>Copy local \u2192 Winston:</p> <pre><code>scp -r &lt;path/to/local/files&gt; &lt;username&gt;@&lt;host-provided-by-admin&gt;:&lt;remote/path&gt;\n</code></pre> <p>Copy Winston \u2192 local (reverse direction):</p> <pre><code>scp -r &lt;username&gt;@&lt;host-provided-by-admin&gt;:&lt;remote/path&gt; &lt;path/to/local/folder&gt;\n</code></pre> <p>Notes: - <code>-r</code> recursively copies directories. - Use absolute paths to avoid surprises (e.g. <code>/data/users/&lt;username&gt;/project</code>).</p>"},{"location":"getting-started/file-transfer/#command-line-rsync-recommended-for-lots-of-files","title":"Command line: <code>rsync</code> (recommended for lots of files)","text":"<p>rsync is faster for large transfers and incremental updates (only changed files are sent).</p> <pre><code>rsync -av &lt;path/to/local/files&gt; &lt;username&gt;@&lt;host-provided-by-admin&gt;:&lt;remote/path&gt;\n</code></pre> <p>Reverse direction (Winston \u2192 local):</p> <pre><code>rsync -av &lt;username&gt;@&lt;host-provided-by-admin&gt;:&lt;remote/path&gt; &lt;path/to/local/folder&gt;\n</code></pre> <p>Common flags: - <code>-a</code> preserve metadata (permissions, timestamps) - <code>-v</code> verbose output - add <code>-z</code> to compress during transfer (useful over slower links) - trailing slash matters: <code>data/</code> copies contents; <code>data</code> copies the folder</p>"},{"location":"getting-started/ssh-tips/","title":"SSH tips","text":""},{"location":"getting-started/ssh-tips/#make-a-shortcut-host-entry","title":"Make a shortcut host entry","text":"<p>If you don\u2019t want to type the IP every time, add a host alias to your ssh config:</p> <pre><code>nano ~/.ssh/config\n</code></pre> <p>Then add:</p> <pre><code>Host winston\n    HostName &lt;host-provided-by-admin&gt;\n    User &lt;username&gt;\n    IdentityFile ~/.ssh/id_ed25519\n    ServerAliveInterval 60\n    ServerAliveCountMax 2\n</code></pre> <p>Then connect with:</p> <pre><code>ssh winston\n</code></pre> <p>Note</p> <p>Remove <code>IdentityFile</code> if you are using password-based login.</p>"},{"location":"reference/commands/","title":"Useful commands","text":""},{"location":"reference/commands/#slurm-essentials","title":"Slurm essentials","text":"<pre><code>squeue # see running/queued jobs\nsqueue -u $USER # see your jobs\nsbatch script.sh # submit job\nsalloc --mem=10G --cpus-per-task=4 --time=01:00:00 # reserve interactive resources\nscancel &lt;JOBID&gt; # cancel job\nsinfo # view partitions / nodes\nsacct -j &lt;JOBID&gt; # view job history\n</code></pre> <p>Node details:</p> <pre><code>scontrol show node &lt;node&gt; # see node description\nscontrol show job &lt;JOBID&gt; # see job details\n</code></pre> <p>Spack essentials:</p> <pre><code>spack search &lt;name&gt;\nspack install &lt;pkg&gt;\nspack load &lt;pkg&gt;\nspack find\nspack unload &lt;pkg&gt;\n</code></pre> <p>Conda essentials:</p> <pre><code>conda create -n myenv python=3.11\nconda activate myenv\nconda deactivate\n</code></pre>"},{"location":"reference/resources/","title":"Cluster resources","text":"<p>Approximate available resources:</p> <ul> <li>~192 vCPUs</li> <li>~250GB RAM</li> <li>2 \u00d7 RTX 6000 Ada (48GB VRAM each)</li> </ul>"},{"location":"reference/resources/#memory-defaults","title":"Memory defaults","text":"<p>By default, Slurm jobs are assigned 1GB RAM per CPU core unless you request more.</p>"},{"location":"reference/resources/#checking-availability-of-resources","title":"Checking availability of resources","text":"<pre><code>winfo\n</code></pre> <p>Note</p> <p><code>winfo</code> is a local helper; you can also use <code>sinfo</code> and <code>squeue</code> for Slurm-wide views.</p>"},{"location":"slurm/batch-jobs/","title":"Batch jobs","text":"<p>Batch jobs are submitted with <code>sbatch</code>. You describe what you need (time, memory, CPUs), and Slurm schedules it.</p>"},{"location":"slurm/batch-jobs/#example-cpu-batch-job","title":"Example: CPU batch job","text":"<p>Create a script, e.g. <code>job.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_python_job\n#SBATCH --output=output_%j.log\n#SBATCH --error=error_%j.log\n#SBATCH --time=01:00:00        # Max walltime (hh:mm:ss)\n#SBATCH --mem=10G              # Memory per job\n#SBATCH --cpus-per-task=4      # Number of CPU cores\n#SBATCH --partition=winston    # Default partition (optional)\n\npython my_script.py\n</code></pre> <p>Submit it:</p> <pre><code>sbatch job.sh\n</code></pre> <p>Check status:</p> <pre><code>squeue\n</code></pre> <p>Walltime default</p> <p>If you do not set --time, the current default is 12 hours. Jobs that exceed their walltime will be terminated by the scheduler.</p> <p>Memory requests</p> <p><code>--mem</code> is total memory for the job. If you prefer per-core memory, use <code>--mem-per-cpu</code>.</p>"},{"location":"slurm/gpu-jobs/","title":"GPU jobs","text":"<p>Use the GPU partition for CUDA workloads.</p>"},{"location":"slurm/gpu-jobs/#partitions","title":"Partitions","text":"<ul> <li><code>winston</code> (default): general CPU queue (use for most jobs)</li> <li><code>winston_gpu</code>: for GPU jobs (CUDA) and privileged access to GPU resources</li> </ul> <p>GPU partition limits</p> <p>The <code>winston_gpu</code> partition has 4 dedicated CPUs and 50GB exclusive RAM. Additional resources may be available depending on cluster load, but are not guaranteed.</p>"},{"location":"slurm/gpu-jobs/#example-gpu-batch-job","title":"Example: GPU batch job","text":"<p>Create a batch script <code>my_gpu_job.sh</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_gpu_job\n#SBATCH --partition=winston_gpu\n#SBATCH --gres=gpu:1\n#SBATCH --time=02:00:00\n#SBATCH --mem=20G\n#SBATCH --cpus-per-task=4\n#SBATCH --output=output_%j.log\n\npython train.py\n</code></pre> <p>Submit with:</p> <pre><code>sbatch my_gpu_job.sh\n</code></pre> <p>Tip</p> <p>Once running, use <code>scontrol show job &lt;JOBID&gt;</code> or <code>nvidia-smi</code> (on the node) to confirm GPU allocation.</p>"},{"location":"slurm/interactive-jobs/","title":"Interactive jobs","text":"<p>Interactive jobs let you reserve resources and work \u201clive\u201d on a compute node.</p>"},{"location":"slurm/interactive-jobs/#request-an-interactive-allocation","title":"Request an interactive allocation","text":"<pre><code>salloc --mem=10G --cpus-per-task=4 --time=01:00:00\n</code></pre> <p>Once allocated, run your commands as normal (on the allocated node).</p> <p>Tip</p> <p>If you need interactive work frequently, consider using tmux so sessions survive disconnects.</p>"},{"location":"slurm/interactive-jobs/#one-shot-interactive-shell","title":"One-shot interactive shell","text":"<p>You can also request a shell in one command:</p> <pre><code>srun --pty --mem=10G --cpus-per-task=4 --time=01:00:00 bash\n</code></pre>"},{"location":"slurm/overview/","title":"Slurm overview","text":"<p>Winston uses Slurm to queue jobs and allocate compute resources fairly.</p> <p>You can use Winston in two main ways:</p> <ol> <li>Batch jobs (preferred): submit a script; Slurm runs it when resources are available.</li> <li>Interactive jobs: reserve resources, then work on a compute node.</li> </ol> <p>Do not run heavy work on the login node</p> <p>Small tests are fine, but intensive workloads must run via Slurm. Login nodes are limited to 2 CPU cores and 4GB RAM.</p>"},{"location":"slurm/overview/#quick-commands","title":"Quick commands","text":"<pre><code>squeue               # view all jobs\nsqueue -u $USER      # view your jobs\nscancel &lt;JOBID&gt;      # cancel a job\nsinfo               # view partitions / nodes\nwinfo               # view allocated resources / cluster usage (local helper)\n</code></pre>"},{"location":"slurm/overview/#partitions-and-priority","title":"Partitions and priority","text":"<ul> <li>Partitions: <code>winston</code> (CPU) and <code>winston_gpu</code> (GPU).</li> <li>QoS priority order: faculty \u2192 postdocs \u2192 PhD students.</li> </ul>"},{"location":"software/conda/","title":"Miniconda","text":"<p>Miniconda is recommended for user-level environments (Python, R packages, data-science tools).</p>"},{"location":"software/conda/#first-time-install","title":"First-time install","text":"<pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3\nsource ~/miniconda3/etc/profile.d/conda.sh\nconda config --set auto_activate_base false\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n</code></pre>"},{"location":"software/conda/#shell-setup","title":"Shell setup","text":"<p>Add to <code>~/.bashrc</code>:</p> <pre><code>if [ -f \"$HOME/miniconda3/etc/profile.d/conda.sh\" ]; then\n    source \"$HOME/miniconda3/etc/profile.d/conda.sh\"\nfi\n</code></pre> <p>Reload your bash profile:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Create and use an environment:</p> <pre><code>conda create -n py311 python=3.11 -y\nconda activate py311\nconda install numpy pandas matplotlib\n</code></pre> <p>Deactivate:</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"software/conda/#reproducibility","title":"Reproducibility","text":"<p>Export a full environment:</p> <pre><code>conda env export &gt; env.yml\n</code></pre> <p>Export only packages you explicitly installed:</p> <pre><code>conda env export --from-history &gt; env.yml\n</code></pre>"},{"location":"software/overview/","title":"Software management overview","text":"<p>Winston is configured for user-managed software installations. You do not need administrator privileges (<code>sudo</code>) to install your own tools.</p> <p>Two supported approaches:</p> <ul> <li>Spack: compiled system software (R, compilers, MPI, scientific libraries)</li> <li>Miniconda: user-level environments (Python/R packages, data science stacks)</li> </ul> <p>Rule of thumb: - Use Spack for system-level or compiled dependencies. - Use Conda for Python/R packages and analysis workflows. - It\u2019s fine to mix them (see Using envs in Slurm).</p>"},{"location":"software/overview/#system-overview","title":"System overview","text":"Layer Tool Purpose Shared base layer Spack (<code>/opt/spack</code>) Core system software (R, compilers, MPI, libraries) User build area <code>/data/spack/users/$USER</code> Your Spack-built software install tree User environments Miniconda (<code>~/miniconda3</code>) Python/R environments + packages Scheduler Slurm Runs jobs on compute nodes"},{"location":"software/slurm-with-envs/","title":"Using Spack/Conda inside Slurm jobs","text":"<p>You can use Spack and Conda together in the same job.</p>"},{"location":"software/slurm-with-envs/#example-batch-script","title":"Example batch script","text":"<p><code>job.sbatch</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --time=00:05:00\n#SBATCH --ntasks=1\n#SBATCH --output=output_%j.log\n\n# Set up Spack + Conda\nsource /opt/spack/share/spack/setup-env.sh\nsource ~/miniconda3/etc/profile.d/conda.sh\n\nconda activate py311\nspack load r\n\nRscript myscript.R\n</code></pre> <p>Submit:</p> <pre><code>sbatch job.sbatch\n</code></pre> <p>Tip</p> <p>Spack is great for optimized compiled libraries; Conda is great for high-level packages. If <code>conda activate</code> fails inside a job, make sure you sourced <code>conda.sh</code> as shown above.</p>"},{"location":"software/spack/","title":"Spack","text":"<p>Spack manages compiled system-level software such as R, compilers, MPI, and scientific libraries.</p>"},{"location":"software/spack/#shell-setup","title":"Shell setup","text":"<p>Add to your <code>~/.bashrc</code> (if not already present):</p> <pre><code># Load Spack\nif [ -f /opt/spack/share/spack/setup-env.sh ]; then\n    source /opt/spack/share/spack/setup-env.sh\nfi\n</code></pre> <p>Reload your shell:</p> <pre><code>source ~/.bashrc\n</code></pre>"},{"location":"software/spack/#installing-and-loading-packages","title":"Installing and loading packages","text":"<p>Example:</p> <pre><code>spack install r@4.4.1\nspack load r@4.4.1\nR --version\n</code></pre> <p>Notes: - Builds go under: <code>/data/spack/users/$USER/opt/spack</code> - Remove availability in your shell with: <code>spack unload &lt;pkg&gt;</code> - List installed packages: <code>spack find</code></p> <p>Note</p> <p>If a package already exists on the system, Spack may use a prebuilt binary automatically.</p>"}]}